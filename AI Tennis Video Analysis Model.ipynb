{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "maBVTl-Sntc6",
        "ep3W6TOyrYLG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3lqzftvwAqMp",
        "outputId": "5fa4ff0e-849c-40da-8f25-d6a35a90ebd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.103-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.8-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.103-py3-none-any.whl (875 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.1/875.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.103 ultralytics-thop-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Player Tracker (player_tracker.py)"
      ],
      "metadata": {
        "id": "CANM-vr9iryY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF3_qh4K8qqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee97514-37b6-4470-b26f-9d5c731ffa1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "class PlayerTracker:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = YOLO(model_path)\n",
        "\n",
        "    def detect_frames(self, frames, read_from_stub=False, stub_path=None):\n",
        "        player_detections = []\n",
        "        if read_from_stub and stub_path is not None:\n",
        "            with open(stub_path,'rb') as f:\n",
        "                player_detections=pickle.load(f)\n",
        "            return player_detections\n",
        "\n",
        "        for frame in frames:\n",
        "            player_dict = self.detect_frame(frame)\n",
        "            player_detections.append(player_dict)\n",
        "\n",
        "        if stub_path is not None:\n",
        "            with open(stub_path,'wb') as f:\n",
        "                pickle.dump(player_detections,f)\n",
        "\n",
        "        return player_detections\n",
        "\n",
        "    def detect_frame(self, frame):\n",
        "        results = self.model.track(frame, persist=True)[0]\n",
        "        id_name_dict = results.names\n",
        "        player_dict = {}\n",
        "        for box in results.boxes:\n",
        "            track_id = int(box.id.tolist()[0])\n",
        "            result = box.xyxy.tolist()[0]\n",
        "            object_cls_id = box.cls.tolist()[0]\n",
        "            object_cls_name = id_name_dict[object_cls_id]\n",
        "            if object_cls_name == \"person\":\n",
        "                player_dict[track_id] = result\n",
        "        return player_dict\n",
        "\n",
        "    def draw_bboxes(self, video_frames, player_detections):\n",
        "        output_video_frames = []\n",
        "        for frame, player_dict in zip(video_frames, player_detections):\n",
        "            # Draw bounding boxes\n",
        "            for track_id, bbox in player_dict.items():\n",
        "                x1, y1, x2, y2 = bbox\n",
        "                cv2.putText(frame, f\"Player ID: {track_id}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)\n",
        "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "            output_video_frames.append(frame)\n",
        "        return output_video_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ball Tracker (ball_tracker.py)"
      ],
      "metadata": {
        "id": "ULePE2dxiwRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "class BallTracker:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = YOLO(model_path)\n",
        "\n",
        "    def interpolate_ball_positions(self, ball_positions):\n",
        "        ball_positions = [x.get(1,[]) for x in ball_positions]\n",
        "        # convert the list into pandas dataframe\n",
        "        df_ball_positions = pd.DataFrame(ball_positions,columns=['x1','y1','x2','y2'])\n",
        "\n",
        "        # interpolate the missing values\n",
        "        df_ball_positions = df_ball_positions.interpolate()\n",
        "        df_ball_positions = df_ball_positions.bfill()\n",
        "\n",
        "        ball_positions = [{1:x} for x in df_ball_positions.to_numpy().tolist()]\n",
        "\n",
        "        return ball_positions\n",
        "\n",
        "    def detect_frames(self, frames, read_from_stub=False, stub_path=None):\n",
        "        ball_detections = []\n",
        "        if read_from_stub and stub_path is not None:\n",
        "            with open(stub_path,'rb') as f:\n",
        "                ball_detections=pickle.load(f)\n",
        "            return ball_detections\n",
        "\n",
        "        for frame in frames:\n",
        "            player_dict = self.detect_frame(frame)\n",
        "            ball_detections.append(player_dict)\n",
        "\n",
        "        if stub_path is not None:\n",
        "            with open(stub_path,'wb') as f:\n",
        "                pickle.dump(ball_detections,f)\n",
        "\n",
        "        return ball_detections\n",
        "\n",
        "    def detect_frame(self, frame):\n",
        "        results = self.model.predict(frame, conf=0.15)[0]\n",
        "        ball_dict = {}\n",
        "        for box in results.boxes:\n",
        "            result = box.xyxy.tolist()[0]\n",
        "            ball_dict[1] = result\n",
        "        return ball_dict\n",
        "\n",
        "    def draw_bboxes(self, video_frames, ball_detections):\n",
        "        output_video_frames = []\n",
        "        for frame, ball_dict in zip(video_frames, ball_detections):\n",
        "            # Draw bounding boxes\n",
        "            for track_id, bbox in ball_dict.items():\n",
        "                x1, y1, x2, y2 = bbox\n",
        "                cv2.putText(frame, f\"Ball ID: {track_id}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
        "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 255), 2)\n",
        "            output_video_frames.append(frame)\n",
        "        return output_video_frames"
      ],
      "metadata": {
        "id": "4KYFAO2Eiyix"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### video_utils.py"
      ],
      "metadata": {
        "id": "maBVTl-Sntc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def read_video(video_path):\n",
        "  cap=cv2.VideoCapture(video_path)\n",
        "  frames=[]\n",
        "  while cap.isOpened():\n",
        "    ret,frame=cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    frames.append(frame)\n",
        "  cap.release()\n",
        "  return frames\n",
        "\n",
        "def save_video(output_video_frames,output_video_path):\n",
        "  height, width = output_video_frames[0].shape[:2]\n",
        "  fourcc=cv2.VideoWriter_fourcc(*'MJPG')\n",
        "  out=cv2.VideoWriter(output_video_path,fourcc,24,(width,height))\n",
        "  for frame in output_video_frames:\n",
        "    out.write(frame)\n",
        "  out.release()"
      ],
      "metadata": {
        "id": "KJvrBNCo_dmb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tennis Court Keypoints Training"
      ],
      "metadata": {
        "id": "ep3W6TOyrYLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tennis_court_det_dataset.zip"
      ],
      "metadata": {
        "id": "2WpcYxC7rd_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import  Dataset, DataLoader\n",
        "from torchvision  import transforms, utils, models\n",
        "\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "metadata": {
        "id": "xRzX7Q7Nrg_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KeypointsDataset(Dataset):\n",
        "  def __init__(self, img_dir, data_file):\n",
        "    self.img_dir = img_dir\n",
        "    with open(data_file, 'r') as f:\n",
        "      self.data = json.load(f)\n",
        "\n",
        "    self.transforms = transforms.Compose([\n",
        "      transforms.ToPILImage(),\n",
        "      transforms.Resize((224, 224)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
        "    h,w = img.shape[:2]\n",
        "\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = self.transforms(img)\n",
        "    kps = np.array(item['kps']).flatten()\n",
        "    kps = kps.astype(np.float32)\n",
        "\n",
        "    kps[::2] = 224.0/w # Adjust x coordinates\n",
        "    kps[1::2] = 224.0/h # Adjust y coordinates\n",
        "\n",
        "    return img, kps"
      ],
      "metadata": {
        "id": "BLd0X4EhrlkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = KeypointsDataset(\"data/images\",'data/data_train.json')\n",
        "val_dataset = KeypointsDataset(\"data/images\",'data/data_val.json')\n",
        "\n",
        "train_loader = DataLoader(train_dataset batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "JjfsTz-UrniZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Model\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 14*2) # Replace the last layer\n",
        "model = model.to(dev)\n",
        "\n",
        "# Training the Model\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  for i, (imgs, kps) in enumerate(train_loader):\n",
        "    imgs = imgs.to(dev)\n",
        "    kps = kps.to(dev)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(imgs)\n",
        "    loss = criterion(outputs, kps)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n",
        "\n",
        "# Saving the Model\n",
        "torch.save(model.state_dict(), 'keypoints_model.pth')"
      ],
      "metadata": {
        "id": "bL-UdFz4rpNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Court Line Detector (court_line_detector.py)"
      ],
      "metadata": {
        "id": "d8KhUaDqoUTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CourtLineDetector:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, 14*2)\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "          transforms.ToPILImage(),\n",
        "          transforms.Resize((224, 224)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def predict(self, image):\n",
        "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image_tensor = self.transforms(img_rgb)\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = self.model(image_tensor)\n",
        "\n",
        "        keypoints = outputs.squeeze().numpy()\n",
        "        org_h, org_w = img_rgb.shape[:2]\n",
        "\n",
        "        keypoints[::2] *= org_w/224.0\n",
        "        keypoints[1::2] *= org_h/224.0\n",
        "\n",
        "        return keypoints\n",
        "\n",
        "    def draw_keypoints(self, image, keypoints):\n",
        "        for i in range(0, len(keypoints), 2):\n",
        "          x, y = int(keypoints[i]), int(keypoints[i+1])\n",
        "\n",
        "          cv2.putText(image, str(i//2), (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "          cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n",
        "        return image\n",
        "\n",
        "    def draw_keypoints_on_video(self, video_frames, keypoints):\n",
        "        output_video_frames = []\n",
        "        for frame in video_frames:\n",
        "          frame = self.draw_keypoints(frame, keypoints)\n",
        "          output_video_frames.append(frame)\n",
        "        return output_video_frames"
      ],
      "metadata": {
        "id": "2ZlHd7ajoTu1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main.py"
      ],
      "metadata": {
        "id": "ztD64giMkgpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_video_path=\"/content/drive/MyDrive/AI_Project_Models/input_video.mp4\"\n",
        "video_frames = read_video(input_video_path)\n",
        "\n",
        "player_tracker = PlayerTracker(model_path='yolov8x')\n",
        "ball_tracker = BallTracker(model_path='/content/drive/MyDrive/AI_Project_Models/yolov5_best.pt')\n",
        "\n",
        "player_detections=player_tracker.detect_frames(video_frames,\n",
        "                                               read_from_stub=True,\n",
        "                                               stub_path='/content/drive/MyDrive/AI_Project_Models/tracker_stubs/player_detections.pkl')\n",
        "ball_detections=ball_tracker.detect_frames(video_frames,\n",
        "                                           read_from_stub=True,\n",
        "                                           stub_path='/content/drive/MyDrive/AI_Project_Models/tracker_stubs/ball_detections.pkl')\n",
        "ball_detections = ball_tracker.interpolate_ball_positions(ball_detections)\n",
        "\n",
        "court_line_detector = CourtLineDetector(model_path='/content/drive/MyDrive/AI_Project_Models/keypoints_model.pth')\n",
        "court_keypoints = court_line_detector.predict(video_frames[0])\n",
        "\n",
        "output_video_frames = player_tracker.draw_bboxes(video_frames,player_detections)\n",
        "output_video_frames = ball_tracker.draw_bboxes(output_video_frames,ball_detections)\n",
        "output_video_frames = court_line_detector.draw_keypoints_on_video(video_frames,court_keypoints)\n",
        "save_video(output_video_frames,\"/content/drive/MyDrive/AI_Project_Models/output_video_interpolation.avi\")"
      ],
      "metadata": {
        "id": "UWqsS15J_THD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556b369f-c95b-4297-f298-33207d24eb96"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 164MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "040lQ7nwnAhp",
        "outputId": "f7d8a0bb-5337-410f-bb70-a19a80fe4df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}